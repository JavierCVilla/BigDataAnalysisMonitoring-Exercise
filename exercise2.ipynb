{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "\n",
    "# A simple helper function to fill a test tree: this makes the example stand-alone.\n",
    "rsops = ROOT.ROOT.RDF.RSnapshotOptions(\n",
    "         \"RECREATE\",       # mode\n",
    "         ROOT.ROOT.kZLIB,  # compression algorithm\n",
    "         1,                # compression level\n",
    "         200,              # autoflush, number of events per cluster\n",
    "         99,               # split level of output tree\n",
    "         0                 # lazy\n",
    "        )\n",
    "\n",
    "def fill_tree(treeName, fileName):\n",
    "    rdf = ROOT.ROOT.RDataFrame(500000)\n",
    "    rdf.Define(\"b1\", \"(double) tdfentry_\")\\\n",
    "       .Define(\"b2\", \"(int) tdfentry_ * tdfentry_\")\\\n",
    "       .Snapshot(treeName, fileName, \"\", rsops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We prepare an input tree to run on\n",
    "fileName = \"exercise2.root\"\n",
    "treeName = \"myTree\"\n",
    "fill_tree(treeName, fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyRDF, python library to run RDataFrame-based analysis on Spark\n",
    "import PyRDF\n",
    "\n",
    "# Setup a Spark session\n",
    "from pyspark import SparkContext\n",
    "\n",
    "context = SparkContext.getOrCreate(conf=swan_spark_conf)\n",
    "context.stop()\n",
    "\n",
    "# Define Spark as the backend for PyRDF\n",
    "PyRDF.use(\"spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We read the tree from the file and create a RDataFrame, a class that\n",
    "# allows us to interact with the data contained in the tree.\n",
    "RDF = PyRDF.RDataFrame\n",
    "d = RDF(treeName, fileName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram with values from column 'b2'\n",
    "hist1 = d.Filter('b1 > 20000 && b1 < 100000')\\\n",
    "         .Histo1D((\"MyHisto\", \"\", 100, 10000, 150000), \"b1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch Spark computation\n",
    "c1 = ROOT.TCanvas()\n",
    "hist1.Draw()\n",
    "c1.Draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
